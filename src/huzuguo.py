# å¯¼åŒ…
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier  # éšæœºæ£®æ—  # AdaBooståˆ†ç±»æ¨¡å‹API
from sklearn.linear_model import LogisticRegression  # é€»è¾‘å›å½’æ¨¡å‹
from xgboost import XGBClassifier,XGBRegressor    # XGBooståˆ†ç±»æ¨¡å‹API
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, \
    classification_report
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import mutual_info_classif
import os
import logging
import matplotlib.pyplot as plt

# =================è®¾ç½®ä¸­æ–‡æ˜¾ç¤º=================
# plt.rcParams['font.family'] = 'SimHei'
# plt.rcParams['font.size'] = 10
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# 1.æ•°æ®å‡†å¤‡
# 1.1è¯»å–æ•°æ®
def load_data(df_train,df_test):
    """
    åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
    :param df_train: è®­ç»ƒé›† DataFrame
    :param df_test: æµ‹è¯•é›† DataFrame
    :return: å¤„ç†åçš„è®­ç»ƒç‰¹å¾ã€æ ‡ç­¾å’Œæµ‹è¯•ç‰¹å¾ã€æ ‡ç­¾
    """
    print(df_train.shape)
    print(df_train.isna().sum())
    print('----------åŠ è½½æ•°æ®----------------')
    # 1.2æå–ç‰¹å¾å’Œæ ‡ç­¾
    X_train = df_train.iloc[:,1:]
    Y_train = df_train.iloc[:,0]
    X_test = df_test.iloc[:,0:-1]
    Y_test = df_test['Attrition']
    X_train = X_train.drop(columns=['EmployeeNumber','Over18', 'StandardHours'])
    X_test = X_test.drop(columns=['EmployeeNumber','Over18', 'StandardHours'])
    print(X_train.shape)
    print(X_test.shape)
    logging.info("æ•°æ®åŠ è½½å®Œæˆ")
    return X_train,X_test,Y_train,Y_test

# label encoderå’Œone encoderåˆ†åˆ«å¯¹ç±»åˆ«æ•°æ®è¿›è¡Œç‰¹å¾ç¼–ç ï¼Œå¤„ç†ç»„åˆåçš„æ•°æ®ç‰¹å¾åå½¢æˆç‰¹å¾å‘é‡
def apply_label_encoding(X_train, X_test):
    """
    å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡Œ Label Encoding
    :param X_train: è®­ç»ƒé›†ç‰¹å¾ DataFrame
    :param X_test: æµ‹è¯•é›†ç‰¹å¾ DataFrame
    :return: ç¼–ç åçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾
    """
    le = LabelEncoder()
    categorical_columns = X_train.select_dtypes(include=['object']).columns  # è·å–ç±»åˆ«åˆ—

    for col in categorical_columns:
        # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä»¥ç¡®ä¿ä¸€è‡´æ€§
        combined = pd.concat([X_train[col], X_test[col]], axis=0)
        le.fit(combined.astype(str))  # æ‹Ÿåˆæ‰€æœ‰å”¯ä¸€å€¼

        # è½¬æ¢è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train[col] = le.transform(X_train[col].astype(str))
        X_test[col] = le.transform(X_test[col].astype(str))
    return X_train, X_test

# 1.3 å¯¹ç‰¹å¾è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼Œå¹¶ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ—å¯¹é½
# def preprocess_data(X_train,X_test):
#     """
#     æ•°æ®é¢„å¤„ç†,å¯¹ç‰¹å¾è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼Œå¹¶ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ—å¯¹é½
#     :param X_train: è®­ç»ƒé›†ç‰¹å¾
#     :param X_test: æµ‹è¯•é›†ç‰¹å¾
#     :param Y_train: è®­ç»ƒé›†æ ‡ç­¾
#     :param Y_test: æµ‹è¯•é›†æ ‡ç­¾
#     :return: å¤„ç†åçš„è®­ç»ƒç‰¹å¾ã€æ ‡ç­¾å’Œæµ‹è¯•ç‰¹å¾ã€æ ‡ç­¾
#     """
#     # 1.3 å¯¹ç‰¹å¾è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼Œå¹¶ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ—å¯¹é½
#     x_train = pd.get_dummies(X_train,drop_first=True)
#     x_test = pd.get_dummies(X_test,drop_first=True)
#     print(x_train.shape,x_train)
#     print('----------ç‰¹å¾å’Œæ ‡ç­¾å¤„ç†----------------')
#     return x_train,x_test

# 1.5 ç‰¹å¾æ ‡å‡†åŒ–
def standardize_features(x_train,x_test):
    """
    å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
    :param x_train: è®­ç»ƒé›†ç‰¹å¾
    :param x_test: æµ‹è¯•é›†ç‰¹å¾
    :return: æ ‡å‡†åŒ–åçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾
    """
    ss = StandardScaler()
    x_train_ss = ss.fit_transform(x_train)
    x_test_ss = ss.transform(x_test)
    return x_train_ss,x_test_ss ,ss


# # 2.æ¨¡å‹è¯„ä¼°
def train_and_evaluate(model, x_train_ss, x_test_ss, Y_train, Y_test, model_name="Model"):
    """
    é€šç”¨æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°å‡½æ•°
    :param model: æ¨¡å‹å¯¹è±¡
    :param x_train_ss: æ ‡å‡†åŒ–åçš„è®­ç»ƒé›†ç‰¹å¾
    :param x_test_ss: æ ‡å‡†åŒ–åçš„æµ‹è¯•é›†ç‰¹å¾
    :param Y_train: è®­ç»ƒé›†æ ‡ç­¾
    :param Y_test: æµ‹è¯•é›†æ ‡ç­¾
    :param model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—è¾“å‡ºï¼‰
    :return: é¢„æµ‹ç»“æœå’Œæ¦‚ç‡
    """
    model.fit(x_train_ss, Y_train)
    y_pred = model.predict(x_test_ss)
    y_pred_proba = model.predict_proba(x_test_ss)[:, 1]

    auc_score = roc_auc_score(Y_test, y_pred_proba)
    f1 = f1_score(Y_test, y_pred)
    print(f"{model_name} AUCåˆ†æ•°: {auc_score}, F1åˆ†æ•°: {f1}")
    return y_pred, y_pred_proba

def plot_feature_importance(x_train_ss, Y_train): # ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
    """
    ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
    :param x_train_ss: æ ‡å‡†åŒ–åçš„è®­ç»ƒé›†ç‰¹å¾
    :param Y_train: è®­ç»ƒé›†æ ‡ç­¾
    :param feature_names: ç‰¹å¾ååˆ—è¡¨
    """
    mutual_info = mutual_info_classif(x_train_ss,Y_train,random_state=666)
    mutual_info_series = pd.Series(mutual_info,index=x_train.columns)
    mutual_info_sorted = mutual_info_series.sort_values(ascending=True)

    plt.figure(figsize=(12, 9))
    plt.title('ç‰¹å¾é‡è¦æ€§',fontsize=10)
    mutual_info_sorted.plot(kind='bar', color='r')
    plt.xlabel('ç‰¹å¾åç§°')
    plt.ylabel('ç‰¹äº’ä¿¡æ¯å¾—åˆ†')
    plt.xticks(rotation=45, ha='right') # æ—‹è½¬xè½´æ ‡ç­¾ä»¥ä¾¿é˜…è¯»
    plt.tight_layout()
    plt.show()

# ------------------- ç½‘æ ¼æœç´¢ä¼˜åŒ–é€»è¾‘å›å½’æ¨¡å‹ -------------------
def optimize_logistic_regression_with_grid_search(x_train_ss, x_test_ss, Y_train, Y_test):
    """
    ä½¿ç”¨ç½‘æ ¼æœç´¢ä¼˜åŒ–é€»è¾‘å›å½’æ¨¡å‹
    :param x_train: è®­ç»ƒé›†ç‰¹å¾
    :param x_test: æµ‹è¯•é›†ç‰¹å¾
    :param Y_train: è®­ç»ƒé›†æ ‡ç­¾
    :param Y_test: æµ‹è¯•é›†æ ‡ç­¾
    :return: æœ€ä¼˜æ¨¡å‹å’Œé¢„æµ‹ç»“æœ
    """
    # å®šä¹‰é€»è¾‘å›å½’æ¨¡å‹
    lr_model = LogisticRegression(random_state=666, max_iter=1000)

    # å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grid = {
        'C': [0.01, 0.1, 1, 10, 100],  # æ­£åˆ™åŒ–å¼ºåº¦
        'penalty': ['l1', 'l2'],       # æ­£åˆ™åŒ–ç±»å‹
        'solver': ['liblinear']}        # ä»…æ”¯æŒ l1 å’Œ l2 çš„æ±‚è§£å™¨

    # ä½¿ç”¨GridSearchCVè¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
    grid_search = GridSearchCV(estimator=lr_model,param_grid=param_grid,scoring='roc_auc',cv=5,verbose=1,n_jobs=-1)

    # æ‹Ÿåˆæ¨¡å‹
    grid_search.fit(x_train_ss, Y_train)

    # è¾“å‡ºæœ€ä¼˜å‚æ•°
    print("æœ€ä¼˜å‚æ•°:", grid_search.best_params_)
    # print("æœ€ä¼˜å¾—åˆ†:", grid_search.best_score_)

    # ä½¿ç”¨æœ€ä¼˜æ¨¡å‹è¿›è¡Œé¢„æµ‹
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(x_test_ss)
    y_pred_proba = best_model.predict_proba(x_test_ss)[:, 1]

    # è¯„ä¼°æ¨¡å‹æ€§èƒ½
    auc_score = roc_auc_score(Y_test, y_pred_proba)
    f1 = f1_score(Y_test, y_pred)
    print(f"ä¼˜åŒ–åé€»è¾‘å›å½’ AUCåˆ†æ•°: {auc_score}, F1åˆ†æ•°: {f1}")

    return best_model, y_pred, y_pred_proba

# ------------------- ç½‘æ ¼æœç´¢ä¼˜åŒ–XGBoostæ¨¡å‹ -------------------
def optimize_xgboost_with_grid_search(x_train_ss, x_test_ss, Y_train, Y_test):
    """
    ä½¿ç”¨ç½‘æ ¼æœç´¢ä¼˜åŒ–XGBoostæ¨¡å‹
    :param x_train: è®­ç»ƒé›†ç‰¹å¾
    :param x_test: æµ‹è¯•é›†ç‰¹å¾
    :param Y_train: è®­ç»ƒé›†æ ‡ç­¾
    :param Y_test: æµ‹è¯•é›†æ ‡ç­¾
    :return: æœ€ä¼˜æ¨¡å‹å’Œé¢„æµ‹ç»“æœ
    """
    # å®šä¹‰XGBoostæ¨¡å‹
    xgb_model = XGBClassifier(random_state=666, eval_metric='logloss')

    # å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grid = {
        'n_estimators': [50,55,60,65,75,90,100,110,120,125,130,135,140,150],'learning_rate': [0.01, 0.1, 0.2],'max_depth': [1,3, 5, 7,9],
        'subsample': [0.8, 1.0]}

    # ä½¿ç”¨GridSearchCVè¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
    grid_search = GridSearchCV(estimator=xgb_model,param_grid=param_grid,scoring='roc_auc',cv=5,verbose=1,n_jobs=-1)

    # æ‹Ÿåˆæ¨¡å‹
    grid_search.fit(x_train_ss, Y_train)

    # è¾“å‡ºæœ€ä¼˜å‚æ•°
    print("æœ€ä¼˜å‚æ•°:", grid_search.best_params_)
    # print("æœ€ä¼˜å¾—åˆ†:", grid_search.best_score_)

    # ä½¿ç”¨æœ€ä¼˜æ¨¡å‹è¿›è¡Œé¢„æµ‹
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(x_test_ss)
    y_pred_proba = best_model.predict_proba(x_test_ss)[:, 1]

    # è¯„ä¼°æ¨¡å‹æ€§èƒ½
    auc_score = roc_auc_score(Y_test, y_pred_proba)
    f1 = f1_score(Y_test, y_pred)
    print(f"ä¼˜åŒ–åXGBoost AUCåˆ†æ•°: {auc_score}, F1åˆ†æ•°: {f1}")

    return best_model, y_pred, y_pred_proba

# ------------------- ç½‘æ ¼æœç´¢ä¼˜åŒ–éšæœºæ£®æ—æ¨¡å‹ -------------------
def optimize_random_forest_with_grid_search(x_train, x_test, Y_train, Y_test):
    """
    ä½¿ç”¨ç½‘æ ¼æœç´¢ä¼˜åŒ–éšæœºæ£®æ—æ¨¡å‹
    :param x_train: è®­ç»ƒé›†ç‰¹å¾
    :param x_test: æµ‹è¯•é›†ç‰¹å¾
    :param Y_train: è®­ç»ƒé›†æ ‡ç­¾
    :param Y_test: æµ‹è¯•é›†æ ‡ç­¾
    :return: æœ€ä¼˜æ¨¡å‹å’Œé¢„æµ‹ç»“æœ
    """
    # å®šä¹‰éšæœºæ£®æ—æ¨¡å‹
    rf_model = RandomForestClassifier(random_state=666)

    # å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grid = {
        'n_estimators': [100, 200, 300],'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],'min_samples_leaf': [1, 2, 4]}

    # ä½¿ç”¨GridSearchCVè¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
    grid_search = GridSearchCV(estimator=rf_model,param_grid=param_grid,
        scoring='roc_auc',cv=5,verbose=1,n_jobs=-1)

    # æ‹Ÿåˆæ¨¡å‹
    grid_search.fit(x_train_ss, Y_train)

    # è¾“å‡ºæœ€ä¼˜å‚æ•°
    print("æœ€ä¼˜å‚æ•°:", grid_search.best_params_)
    # print("æœ€ä¼˜å¾—åˆ†:", grid_search.best_score_)

    # ä½¿ç”¨æœ€ä¼˜æ¨¡å‹è¿›è¡Œé¢„æµ‹
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(x_test_ss)
    y_pred_proba = best_model.predict_proba(x_test_ss)[:, 1]

    # è¯„ä¼°æ¨¡å‹æ€§èƒ½
    auc_score = roc_auc_score(Y_test, y_pred_proba)
    f1 = f1_score(Y_test, y_pred)
    print(f"ä¼˜åŒ–åéšæœºæ£®æ— AUCåˆ†æ•°: {auc_score}, F1åˆ†æ•°: {f1}")

    return best_model, y_pred, y_pred_proba

# ------------------- ç½‘æ ¼æœç´¢ä¼˜åŒ–AdaBoostæ¨¡å‹ -------------------
def optimize_adaboost_with_grid_search(x_train, x_test, Y_train, Y_test):
    """
    ä½¿ç”¨ç½‘æ ¼æœç´¢ä¼˜åŒ–AdaBoostæ¨¡å‹
    :param x_train: è®­ç»ƒé›†ç‰¹å¾
    :param x_test: æµ‹è¯•é›†ç‰¹å¾
    :param Y_train: è®­ç»ƒé›†æ ‡ç­¾
    :param Y_test: æµ‹è¯•é›†æ ‡ç­¾
    :return: æœ€ä¼˜æ¨¡å‹å’Œé¢„æµ‹ç»“æœ
    """
    # å®šä¹‰AdaBoostæ¨¡å‹
    ada_model = AdaBoostClassifier(random_state=666)

    # å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grid = {'n_estimators': [50,75, 100.125,150],'learning_rate': [0.01, 0.1, 1.0]}

    # ä½¿ç”¨GridSearchCVè¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
    grid_search = GridSearchCV(estimator=ada_model,param_grid=param_grid,
        scoring='roc_auc',cv=5,verbose=1,n_jobs=-1)

    # æ‹Ÿåˆæ¨¡å‹
    grid_search.fit(x_train_ss, Y_train)

    # è¾“å‡ºæœ€ä¼˜å‚æ•°
    print("æœ€ä¼˜å‚æ•°:", grid_search.best_params_)
    # print("æœ€ä¼˜å¾—åˆ†:", grid_search.best_score_)

    # ä½¿ç”¨æœ€ä¼˜æ¨¡å‹è¿›è¡Œé¢„æµ‹
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(x_test_ss)
    y_pred_proba = best_model.predict_proba(x_test_ss)[:, 1]

    # è¯„ä¼°æ¨¡å‹æ€§èƒ½
    auc_score = roc_auc_score(Y_test, y_pred_proba)
    f1 = f1_score(Y_test, y_pred)
    print(f"ä¼˜åŒ–åAdaBoost AUCåˆ†æ•°: {auc_score}, F1åˆ†æ•°: {f1}")

    return best_model, y_pred, y_pred_proba




# ------------------- ä¸»ç¨‹åºå…¥å£ -------------------
# ============ æ–°å¢ï¼šROCæ›²çº¿åˆå¹¶ç»˜åˆ¶å‡½æ•° ============

def plot_merged_roc_curves(y_true, predictions_dict, model_names_dict=None,
                           save_path='../data/picture/äººæ‰æµå¤±_å¤šæ¨¡å‹ROCåˆå¹¶æ›²çº¿.png',
                           figsize=(14, 10), show_best_threshold=True):
    """
    åˆå¹¶ç»˜åˆ¶å¤šä¸ªæ¨¡å‹çš„ROCæ›²çº¿å¯¹æ¯”å›¾

    å‚æ•°:
    ----------
    y_true : array-like
        çœŸå®æ ‡ç­¾
    predictions_dict : dict
        é¢„æµ‹æ¦‚ç‡å­—å…¸ï¼Œæ ¼å¼: {'æ¨¡å‹å': y_pred_proba}
    model_names_dict : dict, optional
        æ¨¡å‹æ˜¾ç¤ºåç§°å­—å…¸ï¼Œç”¨äºç¾åŒ–æ˜¾ç¤º
    save_path : str
        ä¿å­˜è·¯å¾„
    figsize : tuple
        å›¾å½¢å°ºå¯¸
    show_best_threshold : bool
        æ˜¯å¦æ˜¾ç¤ºæœ€ä½³é˜ˆå€¼ç‚¹

    è¿”å›:
    -------
    dict
        å„æ¨¡å‹çš„è¯¦ç»†è¯„ä¼°ç»“æœ
    """
    from sklearn.metrics import roc_curve, auc, roc_auc_score
    import matplotlib.pyplot as plt
    import numpy as np
    import os
    import pandas as pd
    plt.rcParams['font.sans-serif'] = [
        'Arial Unicode MS',  # macOS è‡ªå¸¦ä¸­æ–‡å­—ä½“
        'PingFang SC',  # è‹¹æ–¹å­—ä½“
        'Hiragino Sans GB',  # å†¬é’é»‘ä½“
        'STHeiti',  # åæ–‡é»‘ä½“
        'Lantinghei SC'  # å…°äº­é»‘
    ]
    plt.rcParams['axes.unicode_minus'] = False
    print("\n" + "=" * 70)
    print("ç»˜åˆ¶å¤šæ¨¡å‹ROCåˆå¹¶æ›²çº¿")
    print("=" * 70)

    # åˆ›å»ºå›¾å½¢
    plt.figure(figsize=figsize)

    # é¢œè‰²å’Œæ ·å¼é…ç½®
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#FFA07A', '#20B2AA']
    line_styles = ['-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 5))]
    markers = ['o', 's', '^', 'D', 'v', 'p', '*', 'X']

    # å­˜å‚¨ç»“æœ
    results = {}
    auc_scores = []

    # ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹çš„ROCæ›²çº¿
    for idx, (model_key, y_pred_proba) in enumerate(predictions_dict.items()):
        try:
            # ä½¿ç”¨ç¾åŒ–åçš„æ¨¡å‹åç§°
            if model_names_dict and model_key in model_names_dict:
                model_name = model_names_dict[model_key]
            else:
                model_name = model_key

            # è®¡ç®—ROCæ›²çº¿
            fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
            roc_auc = auc(fpr, tpr)
            auc_score = roc_auc_score(y_true, y_pred_proba)

            # è®¡ç®—æœ€ä½³é˜ˆå€¼ï¼ˆYouden's JæŒ‡æ•°ï¼‰
            youden_j = tpr - fpr
            best_idx = np.argmax(youden_j)
            best_threshold = thresholds[best_idx]

            # é€‰æ‹©é¢œè‰²å’Œæ ·å¼
            color = colors[idx % len(colors)]
            line_style = line_styles[idx % len(line_styles)]
            marker = markers[idx % len(markers)]

            # ç»˜åˆ¶ROCæ›²çº¿
            plt.plot(fpr, tpr,
                     color=color,
                     linestyle=line_style,
                     linewidth=2.5,
                     alpha=0.85,
                     label=f'{model_name} (AUC={auc_score:.3f})')

            # æ ‡è®°æœ€ä½³é˜ˆå€¼ç‚¹
            if show_best_threshold:
                plt.scatter(fpr[best_idx], tpr[best_idx],
                            color=color,
                            s=100,
                            marker=marker,
                            edgecolors='black',
                            linewidth=1.5,
                            zorder=5,
                            alpha=0.9,
                            label=f'{model_name}æœ€ä½³é˜ˆå€¼ç‚¹' if idx == 0 else "")

            # ä¿å­˜ç»“æœ
            results[model_name] = {
                'fpr': fpr,
                'tpr': tpr,
                'thresholds': thresholds,
                'roc_auc': roc_auc,
                'auc_score': auc_score,
                'best_threshold': best_threshold,
                'best_tpr': tpr[best_idx],
                'best_fpr': fpr[best_idx],
                'color': color,
                'line_style': line_style
            }

            auc_scores.append(auc_score)
            print(f"  âœ… {model_name}: AUC = {auc_score:.4f}, æœ€ä½³é˜ˆå€¼ = {best_threshold:.3f}")

        except Exception as e:
            print(f"  âŒ {model_key} ç»˜åˆ¶å¤±è´¥: {str(e)[:50]}")
            continue

    # ç»˜åˆ¶éšæœºçŒœæµ‹çº¿
    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.6, label='éšæœºçŒœæµ‹ (AUC=0.5000)')

    # è®¾ç½®å›¾å½¢å±æ€§
    plt.xlim([-0.02, 1.02])
    plt.ylim([-0.02, 1.02])
    plt.xlabel('å‡æ­£ä¾‹ç‡ (False Positive Rate)', fontsize=12)
    plt.ylabel('çœŸæ­£ä¾‹ç‡ (True Positive Rate)', fontsize=12)
    plt.title('äººæ‰æµå¤±é¢„æµ‹æ¨¡å‹ROCæ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold', pad=15)
    plt.legend(loc="lower right", fontsize=10)
    plt.grid(True, alpha=0.3, linestyle='--')

    # æ·»åŠ æ€§èƒ½åŒºåŸŸé˜´å½±
    plt.fill_between([0, 1], [0, 0.7], [0.7, 0.7], alpha=0.05, color='red', label='å·® (AUC<0.7)')
    plt.fill_between([0, 1], [0.7, 0.8], [0.8, 0.8], alpha=0.05, color='orange', label='ä¸€èˆ¬ (0.7â‰¤AUC<0.8)')
    plt.fill_between([0, 1], [0.8, 0.9], [0.9, 0.9], alpha=0.05, color='yellow', label='è‰¯å¥½ (0.8â‰¤AUC<0.9)')
    plt.fill_between([0, 1], [0.9, 1.0], [1.0, 1.0], alpha=0.05, color='green', label='ä¼˜ç§€ (AUCâ‰¥0.9)')

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    if results:
        best_model = max(results.items(), key=lambda x: x[1]['auc_score'])[0]
        best_auc = results[best_model]['auc_score']
        avg_auc = np.mean(auc_scores)

        stats_text = (f'ğŸ“Š æ¨¡å‹æ€§èƒ½ç»Ÿè®¡\n'
                      f'â€¢ æ¨¡å‹æ•°é‡: {len(results)}\n'
                      f'â€¢ æœ€ä½³æ¨¡å‹: {best_model}\n'
                      f'â€¢ æœ€ä½³AUC: {best_auc:.4f}\n'
                      f'â€¢ å¹³å‡AUC: {avg_auc:.4f}\n'
                      f'â€¢ æ ·æœ¬æ•°é‡: {len(y_true)}\n'
                      f'â€¢ ç¦»èŒç‡: {y_true.mean():.2%}')

        plt.text(0.95, 0.05, stats_text,
                 transform=plt.gca().transAxes,
                 fontsize=10,
                 verticalalignment='bottom',
                 horizontalalignment='right',
                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.9))

    # ä¿å­˜å›¾å½¢
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
    plt.show()

    print(f"\nğŸ“ˆ åˆ†æå®Œæˆ!")
    print(f"âœ… åˆå¹¶å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    print("=" * 70)

    return results


# ============ æ–°å¢ï¼šä¿å­˜è¯¦ç»†è¯„ä¼°æŠ¥å‘Šå‡½æ•° ============

def save_model_evaluation_report(results, y_test, save_path='../data/picture/äººæ‰æµå¤±æ¨¡å‹è¯„ä¼°æŠ¥å‘Š.csv'):
    """
    ä¿å­˜æ¨¡å‹è¯„ä¼°è¯¦ç»†æŠ¥å‘Š
    """
    import pandas as pd

    report_data = []

    for model_name, result in results.items():
        report_data.append({
            'æ¨¡å‹åç§°': model_name,
            'AUCåˆ†æ•°': result['auc_score'],
            'æœ€ä½³é˜ˆå€¼': result['best_threshold'],
            'çœŸæ­£ä¾‹ç‡(TPR)': result['best_tpr'],
            'å‡æ­£ä¾‹ç‡(FPR)': result['best_fpr'],
            'ç‰¹å¼‚åº¦': 1 - result['best_fpr'],
            'Youden JæŒ‡æ•°': result['best_tpr'] - result['best_fpr']
        })

    report_df = pd.DataFrame(report_data)
    report_df = report_df.sort_values('AUCåˆ†æ•°', ascending=False)

    # æ·»åŠ æ€§èƒ½è¯„çº§
    def get_performance_rating(auc_score):
        if auc_score >= 0.9:
            return 'â˜…â˜…â˜…â˜…â˜… (ä¼˜ç§€)'
        elif auc_score >= 0.8:
            return 'â˜…â˜…â˜…â˜… (è‰¯å¥½)'
        elif auc_score >= 0.7:
            return 'â˜…â˜…â˜… (ä¸€èˆ¬)'
        else:
            return 'â˜…â˜… (éœ€æ”¹è¿›)'

    report_df['æ€§èƒ½è¯„çº§'] = report_df['AUCåˆ†æ•°'].apply(get_performance_rating)

    # ä¿å­˜æŠ¥å‘Š
    report_df.to_csv(save_path, index=False, encoding='utf-8-sig')

    print("ğŸ“‹ è¯¦ç»†è¯„ä¼°æŠ¥å‘Š:")
    print(report_df.to_string(index=False))
    print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")

    return report_df


# ============ ä¿®æ”¹ä¸»ç¨‹åºï¼šæ·»åŠ ROCåˆå¹¶æ›²çº¿ ============

# ------------------- ä¸»ç¨‹åºå…¥å£ -------------------
if __name__ == '__main__':
    df_train = pd.read_csv('../data/train.csv', sep=',')
    df_test = pd.read_csv('../data/test2.csv', sep=',')

    # æ•°æ®å‡†å¤‡
    X_train, X_test, Y_train, Y_test = load_data(df_train, df_test)
    x_train, x_test = apply_label_encoding(X_train, X_test)
    x_train_ss, x_test_ss, ss = standardize_features(x_train, x_test)

    # ============ åŸºç¡€æ¨¡å‹è®­ç»ƒ ============
    print("\n" + "=" * 70)
    print("åŸºç¡€æ¨¡å‹è®­ç»ƒ")
    print("=" * 70)

    # æ¨¡å‹è®­ç»ƒï¼šé€»è¾‘å›å½’
    lr_model = LogisticRegression(random_state=666)
    y_pred_lr, y_pred_proba_lr = train_and_evaluate(lr_model, x_train_ss, x_test_ss, Y_train, Y_test, "é€»è¾‘å›å½’")

    # æ¨¡å‹è®­ç»ƒï¼šXGBoost
    xgb_model = XGBClassifier(n_estimators=100, random_state=666, learning_rate=0.1, use_label_encoder=False)
    y_pred_xgb, y_pred_proba_xgb = train_and_evaluate(xgb_model, x_train_ss, x_test_ss, Y_train, Y_test, "XGBoost")

    # æ¨¡å‹è®­ç»ƒï¼šéšæœºæ£®æ—
    rf_model = RandomForestClassifier(n_estimators=100, random_state=666, max_depth=None)
    y_pred_rf, y_pred_proba_rf = train_and_evaluate(rf_model, x_train_ss, x_test_ss, Y_train, Y_test, "éšæœºæ£®æ—")

    # æ¨¡å‹è®­ç»ƒï¼šå†³ç­–æ ‘
    dt_model = DecisionTreeClassifier(random_state=666)
    y_pred_dt, y_pred_proba_dt = train_and_evaluate(dt_model, x_train_ss, x_test_ss, Y_train, Y_test, "å†³ç­–æ ‘")

    # æ¨¡å‹è®­ç»ƒï¼šAdaBoost
    ada_model = AdaBoostClassifier(n_estimators=50, random_state=666, learning_rate=0.1)
    y_pred_ada, y_pred_proba_ada = train_and_evaluate(ada_model, x_train_ss, x_test_ss, Y_train, Y_test, "AdaBoost")

    # ============ ä¼˜åŒ–æ¨¡å‹è®­ç»ƒ ============
    print("\n" + "=" * 70)
    print("ä¼˜åŒ–æ¨¡å‹è®­ç»ƒ")
    print("=" * 70)

    # ç½‘æ ¼æœç´¢ä¼˜åŒ–é€»è¾‘å›å½’æ¨¡å‹
    best_lr_model, y_pred_lr_optimized, y_pred_proba_lr_optimized = optimize_logistic_regression_with_grid_search(
        x_train_ss, x_test_ss, Y_train, Y_test)

    # ç½‘æ ¼æœç´¢ä¼˜åŒ–XGBoostæ¨¡å‹
    best_xgb_model, y_pred_xgb_optimized, y_pred_proba_xgb_optimized = optimize_xgboost_with_grid_search(
        x_train_ss, x_test_ss, Y_train, Y_test)

    # ç½‘æ ¼æœç´¢ä¼˜åŒ–éšæœºæ£®æ—æ¨¡å‹
    best_rf_model, y_pred_rf_optimized, y_pred_proba_rf_optimized = optimize_random_forest_with_grid_search(
        x_train_ss, x_test_ss, Y_train, Y_test)

    # ç½‘æ ¼æœç´¢ä¼˜åŒ–AdaBoostæ¨¡å‹
    best_ada_model, y_pred_ada_optimized, y_pred_proba_ada_optimized = optimize_adaboost_with_grid_search(
        x_train_ss, x_test_ss, Y_train, Y_test)

    # ============ ç‰¹å¾é‡è¦æ€§å¯è§†åŒ– ============
    print("\n" + "=" * 70)
    print("ç‰¹å¾é‡è¦æ€§åˆ†æ")
    print("=" * 70)
    plot_feature_importance(x_train_ss, Y_train)

    # ============ ç»˜åˆ¶ROCåˆå¹¶æ›²çº¿ ============
    print("\n" + "=" * 70)
    print("å¤šæ¨¡å‹ROCæ›²çº¿åˆå¹¶åˆ†æ")
    print("=" * 70)

    # å‡†å¤‡æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡
    predictions_dict = {
        'é€»è¾‘å›å½’': y_pred_proba_lr,
        'XGBoost': y_pred_proba_xgb,
        'éšæœºæ£®æ—': y_pred_proba_rf,
        'å†³ç­–æ ‘': y_pred_proba_dt,
        'AdaBoost': y_pred_proba_ada,
        'ä¼˜åŒ–é€»è¾‘å›å½’': y_pred_proba_lr_optimized,
        'ä¼˜åŒ–XGBoost': y_pred_proba_xgb_optimized,
        'ä¼˜åŒ–éšæœºæ£®æ—': y_pred_proba_rf_optimized,
        'ä¼˜åŒ–AdaBoost': y_pred_proba_ada_optimized
    }

    # ç¾åŒ–æ¨¡å‹åç§°æ˜¾ç¤º
    model_names_dict = {
        'é€»è¾‘å›å½’': 'é€»è¾‘å›å½’ (åŸºç¡€)',
        'XGBoost': 'XGBoost (åŸºç¡€)',
        'éšæœºæ£®æ—': 'éšæœºæ£®æ— (åŸºç¡€)',
        'å†³ç­–æ ‘': 'å†³ç­–æ ‘ (åŸºç¡€)',
        'AdaBoost': 'AdaBoost (åŸºç¡€)',
        'ä¼˜åŒ–é€»è¾‘å›å½’': 'é€»è¾‘å›å½’ (ä¼˜åŒ–)',
        'ä¼˜åŒ–XGBoost': 'XGBoost (ä¼˜åŒ–)',
        'ä¼˜åŒ–éšæœºæ£®æ—': 'éšæœºæ£®æ— (ä¼˜åŒ–)',
        'ä¼˜åŒ–AdaBoost': 'AdaBoost (ä¼˜åŒ–)'
    }

    # ç»˜åˆ¶åˆå¹¶ROCæ›²çº¿
    roc_results = plot_merged_roc_curves(
        y_true=Y_test,
        predictions_dict=predictions_dict,
        model_names_dict=model_names_dict,
        save_path='../data/picture/äººæ‰æµå¤±_å¤šæ¨¡å‹ROCåˆå¹¶æ›²çº¿.png',
        figsize=(16, 12),
        show_best_threshold=True
    )

    # ============ ä¿å­˜è¯¦ç»†è¯„ä¼°æŠ¥å‘Š ============
    print("\n" + "=" * 70)
    print("ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š")
    print("=" * 70)

    if roc_results:
        report_df = save_model_evaluation_report(
            roc_results,
            Y_test,
            save_path='../data/picture/äººæ‰æµå¤±æ¨¡å‹è¯„ä¼°æŠ¥å‘Š.csv'
        )

        # è¾“å‡ºæœ€ä½³æ¨¡å‹æ¨è
        best_model_row = report_df.iloc[0]
        print("\n" + "=" * 70)
        print("ğŸ† æœ€ä½³æ¨¡å‹æ¨è")
        print("=" * 70)
        print(f"æ¨¡å‹åç§°: {best_model_row['æ¨¡å‹åç§°']}")
        print(f"AUCåˆ†æ•°: {best_model_row['AUCåˆ†æ•°']:.4f}")
        print(f"æ€§èƒ½è¯„çº§: {best_model_row['æ€§èƒ½è¯„çº§']}")
        print(f"å»ºè®®é˜ˆå€¼: {best_model_row['æœ€ä½³é˜ˆå€¼']:.3f}")
        print(f"é¢„æµ‹æ€§èƒ½:")
        print(f"  â€¢ çœŸæ­£ä¾‹ç‡(TPR): {best_model_row['çœŸæ­£ä¾‹ç‡(TPR)']:.3f}")
        print(f"  â€¢ å‡æ­£ä¾‹ç‡(FPR): {best_model_row['å‡æ­£ä¾‹ç‡(FPR)']:.3f}")
        print(f"  â€¢ ç‰¹å¼‚åº¦: {best_model_row['ç‰¹å¼‚åº¦']:.3f}")
        print("=" * 70)

    print("\nğŸ‰ äººæ‰æµå¤±é¢„æµ‹æ¨¡å‹åˆ†æå®Œæˆ!")
























    # ç½‘æ ¼æœç´¢ä¼˜åŒ–XGBoostæ¨¡å‹
    best_xgb_model, y_pred_xgb_optimized, y_pred_proba_xgb_optimized = optimize_xgboost_with_grid_search(
    x_train_ss, x_test_ss, Y_train, Y_test)

    # ç½‘æ ¼æœç´¢ä¼˜åŒ–éšæœºæ£®æ—æ¨¡å‹
    best_rf_model, y_pred_rf_optimized, y_pred_proba_rf_optimized = optimize_random_forest_with_grid_search(
    x_train_ss, x_test_ss, Y_train, Y_test)

    # ç½‘æ ¼æœç´¢ä¼˜åŒ–AdaBoostæ¨¡å‹
    best_ada_model, y_pred_ada_optimized, y_pred_proba_ada_optimized = optimize_adaboost_with_grid_search(
    x_train_ss, x_test_ss, Y_train, Y_test)
